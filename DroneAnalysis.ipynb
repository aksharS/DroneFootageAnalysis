{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import grip\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/SampleVid.mp4\")\n",
    "REFERENCE_INSPECTION_FRAME = cv2.imread(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/blankFrame.jpg\")\n",
    "CROPPED_REFERENCE = cv2.imread(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/RefenceUnderJoystick.jpg\")\n",
    "INSPECTION_FRAME_SIMILARITY_THRESHOLD = 60\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "def isInspectionMode(frame):\n",
    "    difference = cv2.absdiff(REFERENCE_INSPECTION_FRAME, frame)\n",
    "    res = difference.astype(np.uint8)\n",
    "    percentage = abs(100 - ((np.count_nonzero(res) * 100)/ res.size))\n",
    "\n",
    "    if percentage >= INSPECTION_FRAME_SIMILARITY_THRESHOLD:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isInspectionClick(frame):\n",
    "    cropped_frame_left = frame[550:650, 20:300]\n",
    "    cropped_frame_right = frame[550:650, 990:1270]\n",
    "    if isInspectionMode(cropped_frame):\n",
    "        time_frame = frame[10:75, 600:700]\n",
    "        time_str = pytesseract.image_to_string(time_frame, config='--psm 6')\n",
    "\n",
    "        difference = cv2.absdiff(CROPPED_REFERENCE, cropped_frame)\n",
    "        res = difference.astype(np.uint8)\n",
    "        percentage = abs(100 - ((np.count_nonzero(res) * 100)/ res.size))\n",
    "\n",
    "        if percentage < 100:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print (\"Not an Inspection Frame\")\n",
    "        return False\n",
    "    \n",
    "def flightTime(frame):\n",
    "    time_frame = frame[10:75, 600:700]\n",
    "    time_str = pytesseract.image_to_string(time_frame, config='--psm 6')\n",
    "    try:\n",
    "        seconds = (time.strptime(time_str, '%M:%S')[4]*60) + time.strptime(time_str, '%M:%S')[5]\n",
    "        return seconds\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "def checkDifference(frame):\n",
    "    print (frame.shape, CROPPED_REFERENCE.shape)\n",
    "    difference = cv2.subtract(frame, CROPPED_REFERENCE)\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    edges = cv2.Canny(difference, low_threshold, high_threshold)\n",
    "    rho = 1  # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "    threshold = 15  # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 200  # minimum number of pixels making up a line\n",
    "    max_line_gap = 200  # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(frame) * 0  # creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n",
    "                        min_line_length, max_line_gap)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "    \n",
    "    # Draw the lines on the  image\n",
    "    return cv2.addWeighted(img, 0.8, line_image, 1, 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "if cap.isOpened(): # Check if camera opened successfully\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            difference = cv2.absdiff(REFERENCE_INSPECTION_FRAME, frame)\n",
    "            res = difference.astype(np.uint8)\n",
    "            percentage = abs(100 - ((np.count_nonzero(res) * 100)/ res.size))\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, str(percentage),(100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.0,(255,0,255),lineType=cv2.LINE_AA)\n",
    "            # cv2.imshow(\"Video\", frame)\n",
    "            \n",
    "            if percentage >= INSPECTION_FRAME_SIMILARITY_THRESHOLD:\n",
    "                \n",
    "            \n",
    "            \n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "else:\n",
    "    print(\"Error opening video stream or file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cropped_frame = frame[550:650, 0:1280]\n",
    "        cv2.imshow(\"cropped frame\", cropped_frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width = int( CROPPED_REFERENCE.shape[0])\n",
    "frame_height =int( CROPPED_REFERENCE.shape[1])\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/InspectionMode.avi', fourcc, 1, (frame_height, frame_width))\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cropped_frame = frame[550:650, 0:1280]\n",
    "        if isInspectionMode(cropped_frame):\n",
    "            out.write(cropped_frame)\n",
    "            print (\"Added frame to video.\")\n",
    "        else:\n",
    "            print (\"Skipped frame\")\n",
    "    else:\n",
    "        break\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# Extracted Seconds from App\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cropped_frame = frame[550:650, 0:1280]\n",
    "        if isInspectionMode(cropped_frame):\n",
    "            time_frame = frame[10:75, 600:700]\n",
    "            time_str = pytesseract.image_to_string(time_frame, config='--psm 6')\n",
    "            \n",
    "            difference = cv2.absdiff(CROPPED_REFERENCE, cropped_frame)\n",
    "            res = difference.astype(np.uint8)\n",
    "            percentage = abs(100 - ((np.count_nonzero(res) * 100)/ res.size))\n",
    "            \n",
    "            cv2.putText(frame, \"%s - percentage. %s - time\" % (percentage, time_str), (0, 0), cv2.FONT_HERSHEY_SIMPLEX, 1.0,(255,0,255),lineType=cv2.LINE_AA)\n",
    "            # cv2.imshow(\"time\", time_frame)\n",
    "            # cv2.imshow(\"frame\", frame)\n",
    "            if percentage < 100:\n",
    "                print (\"Writing frame %d to disk with percent similarity of %d\" % (count, percentage))\n",
    "                cv2.imwrite(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/frame%d.jpg\" % count, frame)\n",
    "                count += 1\n",
    "            print (percentage, count)\n",
    "        \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        if isInspectionClick(frame):\n",
    "            cv2.imwrite(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/InspectionClickFrame%d.jpg\" % count, frame)\n",
    "            print (flightTime(frame))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 280, 3) (100, 1280, 3)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/core/src/arithm.cpp:659: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5238d7f872f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtime_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misInspectionMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/Lines%d.jpg\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckDifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_frame_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6cda82f0d4fe>\u001b[0m in \u001b[0;36mcheckDifference\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheckDifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCROPPED_REFERENCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdifference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCROPPED_REFERENCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mlow_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mhigh_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/core/src/arithm.cpp:659: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cropped_frame_left = frame[550:650, 20:300]\n",
    "        cropped_frame_right = frame[550:650, 990:1270]\n",
    "        time_frame = frame[10:75, 600:700]\n",
    "        if isInspectionMode(frame):\n",
    "            cv2.imwrite(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/Lines%d.jpg\" % count, checkDifference(cropped_frame_left))\n",
    "            count += 1\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv2.imwrite(\"/Users/akshar/Downloads/Humans in Autonomy/DroneFootageAnalysis/RefenceUnderJoystick.jpg\", cropped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
